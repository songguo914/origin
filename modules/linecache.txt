
linecache模块是支持行式读取的函数库，允许从任何文件里得到任何的行，并且使用缓存进行优化，常见的情况是从单个文件读取多行。
linecache预先把文件读入缓存起来，后面如果你访问该文件的话就不再从硬盘读取，所以经常用于那些读取频率很高的文件。


linecache函数列表:
checkcache(filename)
Discard cache entries that are out of date.
(This is not checked upon each call!)
检查缓存的有效性。如果在缓存中的文件在硬盘上发生了变化，并且你需要更新版本，使用这个函数。如果省略filename，将检查缓存里的所有条目。
  
clearcache()
Clear the cache entirely.
清除缓存。如果你不再需要先前从getline()中得到的行
  
getline(filename, lineno)
从名为filename的文件中得到第lineno行。这个函数从不会抛出一个异常–产生错误时它将返回”（换行符将包含在找到的行里）。如果文件没有找到，这个函数将会在sys.path搜索。
 
getlines(filename)
Get the lines for a file from the cache.
Update the cache if it doesn't contain an entry for this file already.
从名为filename的文件中得到全部内容，输出为列表格式，以文件每行为列表中的一个元素,并以linenum-1为元素在列表中的位置存储
  
updatecache(filename)
Update a cache entry and return its list of lines.
If something's wrong, print a message, discard the cache entry,
and return an empty list.
更新文件名为filename的缓存。如果filename文件更新了，使用这个函数可以更新linecache.getlines(filename)返回的列表。

注意：使用linecache.getlines('a.txt')打开文件的内容之后，如果a.txt文件发生了改变，如你再次用linecache.getlines获取的内容，不是文件的最新内容，还是之前的内容，此时有两种方法：
1、使用linecache.checkcache(filename)来更新文件在硬盘上的缓存，然后再执行linecache.getlines('a.txt')就可以获取到a.txt的最新内容；

2、直接使用linecache.updatecache('a.txt')，即可获取最新的a.txt的最新内容

另：读取文件之后你不需要使用文件的缓存时需要在最后清理一下缓存，使linecache.clearcache()清理缓存，释放缓存。

这个模块是使用内存来缓存你的文件内容，所以需要耗费内存，打开文件的大小和打开速度和你的内存大小有关系。

  
# -*- coding: cp936 -*-
import urllib
import os,sys
import linecache

strFileName = "C:\\a.txt"
Lines= linecache.getlines(strFileName)
  
for intX in range(len(Lines)):
         print Lines[intX]
#打印出Lines行式保存的文件内容




用法举例：
# cat a.txt
1a
2b
3c
4d
5e
6f
7g
1、获取a.txt文件的内容
>>> a=linecache.getlines('a.txt')
>>> a
['1a\n', '2b\n', '3c\n', '4d\n', '5e\n', '6f\n', '7g\n']

2、获取a.txt文件中第1-4行的内容
>>> a=linecache.getlines('a.txt')[0:4]
>>> a
['1a\n', '2b\n', '3c\n', '4d\n']

3、获取a.txt文件中第4行的内容
>>> a=linecache.getline('a.txt',4)
>>> a
'4d\n'


标  题: 请教linecache的并行问题
我用的是一个12个cpu,196G的cluster.由于需要反复在一个超大文件里面不断读取一些行进行处理,于是用上linecache.缓存大概需要70G.同时,为了能减少运算时间,所以打算用10个threads:
####################
ext_ls = ['aa','ab','ac','ad','ae','af','ag','ah','ai','aj']
i=0
T_correct_for_piece_ls=[]
for ext in ext_ls:
     T_correct_for_piece_ls.append( threading.Thread(target=runpiece, args=(ext,)) )
     T_correct_for_piece_ls[i].start()
     i+=1
for T in T_correct_for_piece_ls:
     T.join()
####################
这样10个线程只能用一个CPU100%.所以还是很慢.
于是我又试用multiprocessing:
####################
ext_ls = ['aa','ab','ac','ad','ae','af','ag','ah','ai','aj']
i=0
for ext in ext_ls:
     p = Process(target=runpiece, args=(ext,) )  
     p.start()
     i+=1
####################
结果出现10个进程,分别用去70G缓存....奇怪的是他们竟然可以共处一个小时左右,然后才崩溃...这是啥子事情啊...抓破脑袋不明白...
  
我该怎么做才能只用耗费70G而同时用上多个CPU来做呀?
用共享内存
