公开课：
网易公开课
cs231n          stanford的计算机视觉大牛李飞飞开的课程，深入浅出的讲解了cnn网络
coursera
edx

卷积网络发展的特别迅速，最早是由Lecun提出来的Lenet成为cnn的鼻祖，接下来他的学生Alex提出了层数更深的Alexnet，然后2013年又提出了VGGnet，有16层和19层两种，这些都只是在层数上面的加深，并没有什么其他的创新，而之后google提出了inception net在网络结构上实现了创新，提出了一种inception的机构，facebook ai 实验室又提出了resnet，残差网络，实现了150层的网络结构可训练化，这些我们之后会慢慢讲到。

目前我调试中遇到了：
a、损失函数不变化、梯度消失、梯度爆炸，通过修改激活函数和优化函数得到解决。另外还使用了BatchNormalization层。
b、欠拟合，通过修改网络结构和优化函数得到解决。有些优化函数可能没找到合适的学习率，损失值一直不变。另外最后的全连接层，可能由于我开始设置过小，准确率偏低。
c、过拟合，我解决过拟合分别使用了数据集扩充、数据增广、增加dropout、提前终止epoch训练等


手机坐标系旋转可能解决不了的问题，人的方向相对地球来说也可能是变的。


0、keras中文文档：https://keras.io/zh/regularizers/
大量keras的例子可在github keras的exsamples中找到：https://github.com/keras-team/keras/tree/master/examples

1、ValueError: Negative dimension size caused by subtracting 3 from 1
（然而keras是不断更新的，下面的两个解决方法过阵子可能就过时了。具体以官方为主，可参考github上该项目的解答）
出现这个错误的原因是图片通道的问题。
也就是”channels_last”和”channels_first”数据格式的问题。
input_shape=(3,150, 150)是theano的写法，而tensorflow需要写出：(150,150,3)。

也可以设置不同的后端来进行调整：
    from keras import backend as K  
    K.set_image_dim_ordering('th')  

    from keras import backend as K  
    K.set_image_dim_ordering('tf') 
或者：model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering="th"))


2、一般选的较小卷积核

3、文章中rnn通常放在网络层的后面部分。通常的全连接层，单层自身神经元无连接，循环层自身神经元互相之间有连接，根据连接方法，rnn有多种方法，但使用最多的为lstm。各种方法可以参看http://www.dataguru.cn/article-11860-1.html，这个暂时不建议细看，能明白它从自身的时间角度进行了考虑就可以了

4、偏置的作用举例：
    改变域值，不然0点永远位于分隔线面上。
    
5、含一个隐藏层的神经网络可以无限逼近任意连续函数，为什么还要增加层数？
    层数越多，对输入特征抽象层次越高。
    
6、在BP神经网络中，输入层和输出层的节点个数都是确定的，而隐含层节点个数不确定，那么应该设置为多少才合适呢？
    实际上，隐含层节点个数的多少对神经网络的性能是有影响的，有一个经验公式可以确定隐含层节点数目，如下
    h=math.sqrt(m+n) + a 其中h为隐含层节点数目，m为输入层节点数目，n为输出层节点数目，a为1-10之间的调节常数

7、神经网络batch size如何确定？
（自己理解）小点可能是好的，但如果太小的话可能无法充分利用电脑的并发，运算效率低。改大之后echo可能需要适当增大。另外batch size最好为2的幂方，如2，4，8，16...

8、Keras输出的loss，val这些值如何保存到文本中：
Keras中的fit函数会返回一个History对象，它的History.history属性会把之前的那些值全保存在里面，如果有验证集的话，也包含了验证集的这些指标变化情况，具体写法：
hist=model.fit(train_set_x,train_set_y,batch_size=256,shuffle=True,nb_epoch=nb_epoch,validation_split=0.1)
with open('log_sgd_big_32.txt','w') as f:
    f.write(str(hist.history))

9、验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的。
在Keras中，验证集的划分只要在fit函数里设置validation_split的值就好了，这个对应了取训练集中百分之几的数据出来当做验证集。
但由于shuffle是在validation _split之后执行的，所以如果一开始训练集没有shuffle的话，有可能使验证集全是负样本。
fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)

测试集的使用只要在evaluate函数里设置就好了。print model.evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)

这里注意evaluate和fit函数的默认batch_size都是32，自己记得修改。

predict(self, x, batch_size=None, verbose=0, steps=None)为输入样本生成输出预测。输入样本逐批处理。

10、dropout可以放在很多类层的后面，用来抑制过拟合现象，常见的可以直接放在Dense层后面，对于在Convolutional和Maxpooling层中dropout应该放置在Convolutional和Maxpooling之间，还是Maxpooling后面的说法，我的建议是试！这两种放置方法我都见过，但是孰优孰劣我也不好说，但是大部分见到的都是放在Convolutional和Maxpooling之间。


11、值得注意的是，最优的模型仍然有可能在训练误差和泛化误差之间存在很大的差距。在这种情况下，我们可以通过收集更多的训练样本来缩小差距。

12、防止过拟合：
防止过拟合常用到的一些方法，如：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout及其它。
在统计和机器学习中，为了避免过拟合现象，需要使用额外的技巧，如交叉验证、提早停止、贝斯信息量准则、赤池信息量准则或模型比较等。
(1)、获取更多数据是解决过拟合最有效的方法。
A、从数据源头获取更多数据
B、根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差。
C、数据增强(DataAugmentation)：通过一定规则扩充数据。例如在物体分类问题里，物体在图像中的位置、姿态、尺度、整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。
D、复制原有数据并加上随机噪声
E、重采样
(2)、使用合适的模型：过拟合主要是有两个原因造成的：数据太少+模型太复杂。所以，我们可以通过使用合适复杂度的模型来防止过拟合的问题，让其足够拟合真正的规则，同时又不至于拟合太多抽样误差。
A、网络结构(Architecture):减少网络的层数、神经元个数等均可以限制网络的拟合能力。
B、训练时间(Earlystopping):对于每个神经元而言，其激活函数在不同区间的性能是不同的。当网络权值较小时，神经元的激活函数工作在线性区，此时神经元的拟合能力较弱(类似线性神经元)。我们在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。
C、限制权值(weight-decay),也叫正则化(regularization)：这类方法直接将权值的大小加入到Cost里，在训练的时候限制权值变大。
D、增加噪声Noise：在输入中加噪声：噪声会随着网络传播，按照权值的平方放大，并传播到输出层，对误差Cost产生影响。在权值上加噪声：在初始化网络的时候，用0均值的高斯分布作为初始化。对网络的响应加噪声：如在前向传播过程中，让神经元的输出变为binary或random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据Hinton说，在测试集上效果会有显著提升。
(3)、结合多种模型：简而言之，训练多个模型，以每个模型的平均输出作为结果。从N个模型里随机选择一个作为输出的期望误差，会比所有模型的平均输出的误差大。
A、Bagging：是分段函数的概念，用不同的模型拟合不同部分的训练集。以随机森林(randforests)为例，就是训练了一堆互不关联的决策树。一般不单独使用神经网络做Bagging。
B、Boosting：既然训练复杂神经网络比较慢，那我们就可以只使用简单的神经网络(层数、神经元数限制等)。通过训练一系列简单的神经网络，加权平均其输出。
C、Dropout：这是一个很高效的方法。在训练时，每次随机忽略隐层的某些节点，这样，我们相当于随机从模型中采样选择模型，同时，由于每个网络只见过一个训练数据(每次都是随机的新网络)，所以类似bagging的做法。
(4)、贝叶斯方法。
在统计学中，过拟合(overfitting)现象是指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后(拟合后)模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中的预期的噪声或错误数量，跟模型错误的数量也有关。

13、机器学习防止过拟合方法
simpler model structure（合适模型）
regularization（正则化）
data augmentation（数据集扩增）
dropout（删除隐藏层结点个数）
Bootstrap/Bagging（封装）
ensemble（集成）
early stopping（提前终止迭代）
utilize invariance（利用不变性）
Bayesian（贝叶斯方法）

14、梯度消失与梯度爆炸
深度神经网络训练的时候，采用的反向传播方式，该方式背后其实是链式求导，计算每层梯度的时候会涉及一些连乘操作，因此如果网络过深，那么如果连乘的因子大部分小于1，最后乘积可能趋于0；另一方面，如果连乘的因子大部分大于1，最后乘积可能趋于无穷。这就是所谓梯度消失与梯度爆炸。
为防止梯度爆炸，一种方式是设置梯度剪切阈值 gradient_clipping_threshold， 一旦梯度超过改值，直接置为该值。

15、当训练数据很小时，很容易使训练集上的误差非常小，此时处于过拟合状态。随着训练数据的增加，训练数据上的误差越来越大，而验证集上的误差越来越小，训练误差和验证误差越来越接近但始终保持 验证误差 > 训练误差。

16、当深度网络较浅时，可能会面临多样本数据无法很好拟合的情况。此时，如果加深网络层数，则可能会使得网络很好地拟合多组训练样本。

17、 transfer learning总结起来就是将已有的网络和训练好的权重一起迁移过来，然后训练过程中只修改最后的全连接层部分的参数，实现最后我们的分类目的。
为什么别人在他的数据集上训练好的网络能够拿给我们直接使用呢？
这里有两点，第一点我们并不是将别人的网络拿过来一点不改进行训练，我们会改掉网络最后的全连接层进行训练，因为我们的分类问题并不一定和原来的分类问题一样；第二点为什么训练好的网络拿来直接应用到我们的数据上可行，因为卷积神经网络其实可以理解为两个部分，前面的卷积部分以及后面的分类部分，而前面的卷积部分主要做的事就是提取特征，而他训练好的网络对于图片的特征提取效果是特别好的，所以我们可以直接将他的网络的卷积部分拿过来提取我们自己图片的特征，而我们自己的数据集要实现的分类就是用我们自己的分类全连接层就可以了。
但是 transfer Learning 并不是万能的，它在相似数据集上的效果才是良好的，比如你用的预训练的参数是自然景物的图片分类得到的，你用这个参数来做人脸的识别，效果可能就没有那么好了，因为人脸的特征提取和自然景物的特征提取是不同的，所以相应的参数训练后也是不同的。如果这个是万能的，那我们就不用研究 deep learning 了，直接训练好一个参数然后应用到所有的数据集上就万事大吉了，但是事实上并没有太好的效果，归根结底这只是一种节约计算资源的做法，即相似的数据可以用训练好的参数，不用再重新训练了。
方法
1.我们可以导入预训练好的网络，将最后的全连接层改成我们自己的，然后开始训练，这样会特别快的收敛，达到 transfer learning 的目的。
2.我们可以锁定前面卷积的参数，而让网络只修改最后的全连接层的参数，这样可以让我们训练时间减少。第一种方法我们训练的时候每个epoch都需要跑一边所有的数据集，将数据集前向传播，通过卷积层到我们的全连接层，然后再输出结果，接着反向传播更新参数，这样是非常浪费计算资源的，所以我们可以将数据集经过一次卷积层得到的结果保存起来，这个结果我们称为特征向量，这样所有的数据集就只需要通过一次卷积网络，大大节约了我们的计算资源。
3.前面我们只使用一种预训练好的网络，其实我们可以使用多个预训练好的网络，将他们并联在一起，数据经过每个网络都会得到特征向量，然后将这些特征向量拼接在一起进入我们的全连接层。

18、直接启动程序会占用所有的gpu资源（实际上一个gpu资源都没用满），
    导致其它人无法再启动程序（此时tensorflow会报错）
所以我们暂时采用分配gpu的办法，后续添加代码控制gpu。
执行方法：CUDA_VISIBLE_DEVICES=1 python run_cnn_main.py
后台运行加上nohup和&
CUDA_VISIBLE_DEVICES=1表示用的是gpu1

19、如何在keras中设定GPU使用的大小
本节来源于：深度学习theano/tensorflow多显卡多人使用问题集（参见：Limit the resource usage for tensorflow backend ・ Issue #1538 ・ fchollet/keras ・ GitHub） 
在使用keras时候会出现总是占满GPU显存的情况，可以通过重设backend的GPU占用情况来进行调节。

import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.3
set_session(tf.Session(config=config))

需要注意的是，虽然代码或配置层面设置了对显存占用百分比阈值，但在实际运行中如果达到了这个阈值，程序有需要的话还是会突破这个阈值。换而言之如果跑在一个大数据集上还是会用到更多的显存。以上的显存限制仅仅为了在跑小数据集时避免对显存的浪费而已。（2017年2月20日补充）

20、训练集：用来学习的样本集，用于分类器参数的拟合
验证集：用来调整分类器超参数的样本集，如在神经网络中选择隐藏层神经元的数量
测试集：仅用于对已经训练好的分类器进行性能评估的样本集

